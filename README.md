# OMG Empathy Challenge 2019

<p>
<a href="https://www.alpha.company/">
<img src="https://static1.squarespace.com/static/59954a9ce45a7c2d145edb08/t/5a862059652dea5036d5b54a/1518739555006/alpha+logo.png?format=750w" width="150">
</a>
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
<a href="http://mirg.city.ac.uk/">
<img src="https://media.founders4schools.org.uk/referrers/generic/2015/11/02/City-University-Logo.jpg" width="250">
</a>
</p>

This is an ensemble multimodal model developed for the [OMG Empathy Challenge 2019](https://www2.informatik.uni-hamburg.de/wtm/omgchallenges/omg_empathy2018_results2018.html#) by the **Alpha - City team** (collaboration of [Telef√≥nica Innovation Alpha, Barcelona, Spain](https://www.alpha.company/), and [MIRG - City, University of London, UK](http://mirg.city.ac.uk/)).

This model integrates predictions from different sources (video, audio, and dialogue transcript). To run the full model, each individual module needs to be run separately and the prediction of each of them integrated using one of the proposed methods (Regression model, Smoothed weighted average model or K-nearest Neighbours Model).

The overall diagram of the model is the following:
<p>
<img src="model.png" width="860">
</p>
