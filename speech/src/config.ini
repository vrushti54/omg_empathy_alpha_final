[model]
# where numpy feature matrices and labels will be saved/loaded
training_predictors_load = C:\omg_empathy_alpha\speech\runs\features\train_X.npy
training_target_load     = C:\omg_empathy_alpha\speech\runs\features\train_y.npy
validation_predictors_load = C:\omg_empathy_alpha\speech\runs\features\val_X.npy
validation_target_load     = C:\omg_empathy_alpha\speech\runs\features\val_y.npy
evaluation_predictors_load = C:\omg_empathy_alpha\speech\runs\features\val_X.npy
evaluation_target_load     = C:\omg_empathy_alpha\speech\runs\features\val_y.npy
reference_predictors_load  = C:\omg_empathy_alpha\speech\runs\features\train_X.npy
last_latent_dim_dir        = C:\omg_empathy_alpha\speech\runs\checkpoints
load_model                 = C:\omg_empathy_alpha\speech\runs\checkpoints\best_model.h5
save_model                 = C:\omg_empathy_alpha\speech\runs\checkpoints\best_model.h5

[preprocessing]
# sequence chunking
sequence_length   = 250
sequence_overlap  = 0.5

# which subjectâ€™s valence do we learn (set to listener for empathy)
target_subject    = all
target_story = all
target_delay = 10

# INPUTS (your dataset folders)
input_audio_folder_t       = C:\omg_data\audio\train
input_audio_folder_v       = C:\omg_data\audio\val
input_annotation_folder_t  = C:\omg_data\annotations\train
input_annotation_folder_v  = C:\omg_data\annotations\val

# OUTPUT feature/target files that preprocessing writes
output_predictors_matrix_t = C:\omg_empathy_alpha\speech\runs\features\train_X.npy
output_target_matrix_t     = C:\omg_empathy_alpha\speech\runs\features\train_y.npy
output_predictors_matrix_v = C:\omg_empathy_alpha\speech\runs\features\val_X.npy
output_target_matrix_v     = C:\omg_empathy_alpha\speech\runs\features\val_y.npy

# feature extraction defaults (safe with our librosa patch)
feature_type    = logmel
n_mels          = 64
win_length_sec  = 0.025
hop_length_sec  = 0.010

[sampling]
sr = 16000

[stft]
# hop size in samples (16000 * 0.04 = 640 for 40ms)
hop_size = 640
