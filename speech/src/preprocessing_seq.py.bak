# preprocessing_seq.py
import os
import numpy as np
import pandas as pd
import utilities_func as uf
import feat_analysis2 as fa
import loadconfig
import configparser
from pathlib import Path

# ----------------------------
# helpers
# ----------------------------
def pick_valence_column(df, target_subject):
    cols = [str(c).strip().lower() for c in df.columns]
    subj = (target_subject or "").strip().lower()
    if subj == "listener":
        prefs = ["listener_valence","listener","valence"]
    elif subj == "speaker":
        prefs = ["speaker_valence","speaker","valence"]
    else:
        prefs = ["valence","listener_valence","speaker_valence","listener","speaker"]
    for p in prefs:
        if p in cols:
            return df[p].astype("float32").values
    # last resort: first numeric col
    for c in cols:
        s = pd.to_numeric(df[c], errors="coerce")
        if s.notna().any():
            return s.fillna(method="ffill").fillna(method="bfill").astype("float32").values
    return None

def best_audio_path(audio_dir: Path, base: str) -> Path:
    # match evaluation: prefer .mp4.wav then .wav
    p1 = audio_dir / f"{base}.mp4.wav"
    if p1.exists():
        return p1
    p2 = audio_dir / f"{base}.wav"
    if p2.exists():
        return p2
    return None

# ----------------------------
# load config
# ----------------------------
config_path = loadconfig.load()
cfg = configparser.ConfigParser()
cfg.read(config_path)

SEQ_LENGTH  = cfg.getint('preprocessing', 'sequence_length')
SEQ_OVERLAP = cfg.getfloat('preprocessing', 'sequence_overlap')

SOUND_FOLDER_T              = cfg.get('preprocessing', 'input_audio_folder_t')
ANNOTATION_FOLDER_T         = cfg.get('preprocessing', 'input_annotation_folder_t')
OUTPUT_PREDICTORS_MATRIX_T  = cfg.get('preprocessing', 'output_predictors_matrix_t')
OUTPUT_TARGET_MATRIX_T      = cfg.get('preprocessing', 'output_target_matrix_t')

SOUND_FOLDER_V              = cfg.get('preprocessing', 'input_audio_folder_v')
ANNOTATION_FOLDER_V         = cfg.get('preprocessing', 'input_annotation_folder_v')
OUTPUT_PREDICTORS_MATRIX_V  = cfg.get('preprocessing', 'output_predictors_matrix_v')
OUTPUT_TARGET_MATRIX_V      = cfg.get('preprocessing', 'output_target_matrix_v')

TARGET_SUBJECT = cfg.get('preprocessing', 'target_subject')  # "all" | "listener" | "speaker" | "1" ...
TARGET_STORY   = cfg.get('preprocessing', 'target_story')    # "all" | "1" | "2" | "4" | "5" | "8"
TARGET_DELAY   = cfg.getint('preprocessing', 'target_delay')

SR       = cfg.getint('sampling', 'sr')       # 16000
HOP_SIZE = cfg.getint('stft', 'hop_size')     # 640

FPS = 25
frames_per_annotation = (SR / FPS) / float(HOP_SIZE)  # 640 / 640 = 1.0
frames_delay = int(TARGET_DELAY * frames_per_annotation)

print(f"[config] SR={SR}, HOP_SIZE={HOP_SIZE}, FPS={FPS}, "
      f"frames_per_annotation={frames_per_annotation:.3f}, frames_delay={frames_delay}")
print(f"[config] TARGET_SUBJECT={TARGET_SUBJECT} TARGET_STORY={TARGET_STORY}")
print(f"[paths] train audio={SOUND_FOLDER_T}")
print(f"[paths] train ann  ={ANNOTATION_FOLDER_T}")

# ----------------------------
# core preprocessing
# ----------------------------
def passes_filters(name: str, target_subj: str, target_story: str) -> bool:
    """name like 'Subject_10_Story_2'"""
    try:
        subj = name.split('Subject_')[1].split('_')[0]
        story = name.split('Story_')[1]
    except Exception:
        return False
    ok_subj  = (target_subj == 'all')  or (subj == str(target_subj))
    ok_story = (target_story == 'all') or (story == str(target_story))
    return ok_subj and ok_story

def segment(features, annotation, seq_len, seq_overlap):
    # advance = seq_len * (1 - overlap); e.g., overlap 0.5 => advance 125 when seq_len=250
    step = max(1, int(round(seq_len * (1.0 - seq_overlap))))
    ptrs = np.arange(0, len(annotation), step, dtype=int)
    X, y = [], []
    for start in ptrs:
        end_a = start + seq_len
        if end_a <= len(annotation):
            start_f = int(start * frames_per_annotation)
            end_f   = int(end_a * frames_per_annotation)
            X.append(features[start_f:end_f])
            y.append(annotation[start:end_a])
        else:
            X.append(features[-int(seq_len*frames_per_annotation):])
            y.append(annotation[-seq_len:])
    return np.array(X, dtype="float32"), np.array(y, dtype="float32")

def preprocess_dataset(audio_dir: str, ann_dir: str, target_subj: str, target_story: str):
    audio_dir = Path(audio_dir); ann_dir = Path(ann_dir)
    csvs = sorted([p for p in ann_dir.glob("*.csv")])
    bases = [p.stem for p in csvs]
    print(f"[debug] found {len(csvs)} CSVs in {ann_dir}")
    if not csvs:
        raise RuntimeError(f"No CSVs found in {ann_dir}")

    kept = [b for b in bases if passes_filters(b, target_subj, target_story)]
    print(f"[debug] after filters (subject={target_subj}, story={target_story}): {len(kept)} files")

    X_list, y_list = [], []
    for i, base in enumerate(kept, 1):
        wav_path = best_audio_path(audio_dir, base)
        csv_path = ann_dir / f"{base}.csv"
        if wav_path is None:
            print(f"[skip] audio missing for {base}")
            continue

        # audio -> preemphasis -> features (explicit hop for safety)
        sr, samples = uf.wavread(str(wav_path))
        e_samples = uf.preemphasis(samples, sr)
        feats = fa.extract_features(e_samples, sr=sr, hop_samples=HOP_SIZE)

        # annotations
        df = pd.read_csv(csv_path)
        val = pick_valence_column(df, TARGET_SUBJECT)
        if val is None:
            print(f"[skip] no usable valence column in {csv_path.name}")
            continue

        # trim to aligned length (defensive)
        annotated_frames = int(round(len(val) * frames_per_annotation))
        T = min(annotated_frames, len(feats))
        feats = feats[:T]
        val_len = int(round(T / frames_per_annotation))
        val = val[:val_len]

        # delay
        if TARGET_DELAY > 0:
            cut_a = min(TARGET_DELAY, len(val))
            cut_f = min(frames_delay, len(feats))
            val = val[cut_a:]
            feats = feats[:len(feats)-cut_f]

        # segment
        X, y = segment(feats, val, SEQ_LENGTH, SEQ_OVERLAP)
        if X.size == 0 or y.size == 0:
            print(f"[warn] empty segments for {base} -> skipping")
            continue

        X_list.append(X); y_list.append(y)
        if i % 5 == 0:
            print(f"[progress] {i}/{len(kept)} processed")

    if not X_list:
        raise RuntimeError("No predictors created. Check that audio/CSV names and hops align.")

    X_all = np.concatenate(X_list, axis=0).astype("float32")
    y_all = np.concatenate(y_list, axis=0).astype("float32")

    # reproducible shuffle
    rng = np.random.RandomState(42)
    idx = rng.permutation(len(y_all))
    X_all = X_all[idx]
    y_all = y_all[idx]

    print(f"[done] predictors shape={X_all.shape}, target shape={y_all.shape}")
    return X_all, y_all

def build_matrices(out_X, out_y, audio_dir, ann_dir):
    X, y = preprocess_dataset(audio_dir, ann_dir, TARGET_SUBJECT, TARGET_STORY)
    np.save(out_X, X)
    np.save(out_y, y)
    print(f"[save] {out_X} and {out_y} written.")

# ----------------------------
# main
# ----------------------------
if __name__ == '__main__':
    build_matrices(OUTPUT_PREDICTORS_MATRIX_T, OUTPUT_TARGET_MATRIX_T, SOUND_FOLDER_T, ANNOTATION_FOLDER_T)
    build_matrices(OUTPUT_PREDICTORS_MATRIX_V, OUTPUT_TARGET_MATRIX_V, SOUND_FOLDER_V, ANNOTATION_FOLDER_V)
