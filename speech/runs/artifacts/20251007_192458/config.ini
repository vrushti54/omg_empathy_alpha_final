[preprocessing]
# input folders
input_audio_folder_t = C:\omg_data\audio\train
input_annotation_folder_t = C:\omg_data\annotations\train

input_audio_folder_v = C:\omg_data\audio\val
input_annotation_folder_v = C:\omg_data\annotations\val

# output feature matrices
output_predictors_matrix_t = C:\omg_empathy_alpha\speech\runs\features\train_X.npy
output_target_matrix_t     = C:\omg_empathy_alpha\speech\runs\features\train_y.npy
output_predictors_matrix_v = C:\omg_empathy_alpha\speech\runs\features\val_X.npy
output_target_matrix_v     = C:\omg_empathy_alpha\speech\runs\features\val_y.npy

# sequence setup
sequence_length  = 250
sequence_overlap = 0.95

# target selection
target_subject = listener
target_story   = all

# delays
target_delay = 40
frames_delay = 40

[training]
hidden_size    = 300
num_layers     = 3
dropout = 0.35
batch_size     = 64
epochs = 10
learning_rate  = 0.0005
alpha_ccc      = 0.8
normalize_mode = z

[paths]
runs_root  = C:\omg_empathy_alpha\speech\runs
models_dir = C:\omg_empathy_alpha\speech\runs\models
plots_dir  = C:\omg_empathy_alpha\speech\runs\plots
eval_dir   = C:\omg_empathy_alpha\speech\runs\eval_results
